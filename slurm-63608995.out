~/src/absa/multi-view-prompting-acosi ~/src/absa
+ cd src
+ python data_process/process_unify.py
task: ../data/acos
data: ../data/acos/laptop16
data: ../data/acos/rest16
task: ../data/asqp
data: ../data/asqp/rest15
data: ../data/asqp/rest16
task: ../data/aste
data: ../data/aste/rest14
data: ../data/aste/rest15
data: ../data/aste/laptop14
data: ../data/aste/rest16
task: ../data/tasd
data: ../data/tasd/rest15
data: ../data/tasd/rest16
task: ../data/acosi
data: ../data/acosi/shoes
test inputs size: 2948
train data size (before remove test): 16069
train data size (after remove test): 14020
train data size (dedup): 10073
Tasks counts: Counter({'acos': 3907, 'aste': 3088, 'tasd': 1166, 'acosi': 1097, 'asqp': 815})
Data counts: Counter({'laptop16': 3237, 'rest16': 2002, 'rest14': 1330, 'rest15': 1287, 'laptop14': 1120, 'shoes': 1097})
+ export CUDA_VISIBLE_DEVICES=0
+ CUDA_VISIBLE_DEVICES=0
+ cd src
+ for SEED in 5
+ K=5
+ INFER_PATH=5
+ CTRL_TOKEN=post
+ TASK=unified
+ OUT_DIR=../outputs/unified/top5_seed5
+ mkdir -p ../outputs/unified/top5_seed5
+ python main.py --data_path ../data/ --dataset seed5 --model_name_or_path t5-base --output_dir ../outputs/unified/top5_seed5 --num_train_epochs 20 --save_top_k 0 --task unified --top_k 5 --ctrl_token post --multi_path --num_path 5 --seed 5 --train_batch_size 16 --gradient_accumulation_steps 1 --learning_rate 1e-4 --lowercase --sort_label --data_ratio 1.0 --check_val_every_n_epoch 10 --agg_strategy vote --eval_batch_size 64 --constrained_decode --multi_task --do_train
Some weights of the model checkpoint at t5-base were not used when initializing MyT5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']
- This IS expected if you are initializing MyT5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing MyT5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of MyT5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.embed_tokens.weight', 'lm_head.weight', 'decoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/qwzhao/.conda/envs/mvp/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python main.py --data_path ../data/ --dataset seed5 --model ...
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/qwzhao/.conda/envs/mvp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
  warning_cache.warn(
You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type                         | Params
-------------------------------------------------------
0 | model | MyT5ForConditionalGeneration | 222 M 
-------------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
/home/qwzhao/.conda/envs/mvp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/qwzhao/.conda/envs/mvp/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
